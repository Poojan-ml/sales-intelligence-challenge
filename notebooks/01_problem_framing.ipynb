{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Part 1 – Problem Framing\n\n**No code required.** This section establishes the business context and analytical framework before touching any data.\n\n---\n\n## 1. What is the Real Business Problem?\n\nThe CRO says: *\"Our win rate has dropped over the last two quarters, but pipeline volume looks healthy. I don't know what exactly is going wrong or what my team should focus on.\"*\n\n### Surface-Level Reading\nWin rate is declining. Fix the win rate.\n\n### Deeper Reading\nThe real problem is **the absence of diagnostic visibility**. The CRO can see *that* something is wrong (the aggregate win rate number) but cannot see *why* it's wrong or *where* to intervene. This is a decision intelligence gap, not just an analytics gap.\n\nSpecifically, three compounding issues are likely at play:\n\n1. **Aggregate metrics mask segment-level problems.** A company-wide win rate of, say, 42% could mean every segment is at 42%, or it could mean one segment is at 60% and another collapsed to 20%. These require completely different responses.\n\n2. **\"Healthy pipeline\" is a misleading signal.** Pipeline volume (deal count or total value) is a vanity metric if the deals entering the pipeline are lower quality, worse-fit, or in segments where the company loses more often. A pipeline full of bad-fit deals is worse than a smaller pipeline of well-qualified ones. Volume can actually *cause* win rate decline if it dilutes rep attention across too many low-probability deals.\n\n3. **The CRO needs *actions*, not charts.** Knowing win rate dropped is useless without knowing: which reps need coaching, which segments to double down on, which deal types to deprioritize, and what the revenue impact is of doing nothing.\n\n**In summary:** The real business problem is that the revenue organization lacks a diagnostic system that connects leading indicators to root causes to recommended actions. The win rate decline is the symptom; the lack of decision intelligence infrastructure is the disease.\n\n---\n\n## 2. What Key Questions Should an AI System Answer for the CRO?\n\nAn effective sales intelligence system should answer questions in a **diagnostic hierarchy** — from confirmation to action:\n\n### Tier 1: Confirm & Quantify\n- Is the win rate decline statistically significant, or is it within normal variance?\n- What is the revenue impact of the decline? (e.g., \"the 8-point drop in win rate cost approximately $X in the last quarter\")\n- Is the decline accelerating, stabilizing, or recovering?\n\n### Tier 2: Localize the Problem\n- Which segments are driving the decline? (industry, region, product tier, lead source)\n- Is it concentrated in a few sales reps, or is it team-wide?\n- Has the *mix* of deals changed? (i.e., are we getting more deals in segments where we historically lose?)\n\n### Tier 3: Diagnose Root Causes\n- Are deals taking longer to close? (sales cycle elongation often precedes win rate drops)\n- Are deals stalling at a specific stage? (e.g., Proposal → Negotiation conversion dropped)\n- Is there a deal-size effect? (chasing bigger deals with lower win probability)\n- Are newer lead sources underperforming? (channel quality degradation)\n\n### Tier 4: Prescribe Actions\n- Which specific behaviors or segments offer the highest leverage for improvement?\n- Which reps need coaching, and on what type of deals?\n- Should the team shift focus toward higher-win-rate segments, even if deal sizes are smaller?\n- What would the revenue impact be if we recovered win rate in the worst-performing segment?\n\n---\n\n## 3. What Metrics Matter Most for Diagnosing Win Rate Issues?\n\n### Standard Metrics (table stakes)\n| Metric | What It Tells You |\n|--------|-------------------|\n| **Win Rate** (overall and by segment) | The headline metric — but only useful when decomposed |\n| **Average Deal Size** (won vs. lost) | Whether we're chasing the wrong deal sizes |\n| **Average Sales Cycle** (won vs. lost) | Whether deals are taking too long (stalling = dying) |\n| **Pipeline Volume** (by quarter) | Whether there's truly \"healthy\" flow |\n| **Stage Conversion Rates** | Where deals are dying in the funnel |\n\n### Diagnostic Metrics (where insight lives)\n| Metric | What It Tells You |\n|--------|-------------------|\n| **Win Rate Trend Slope** (by segment) | Not just \"is it low\" but \"is it getting worse\" |\n| **Pipeline Mix Shift** | Whether the *composition* of deals has changed (more deals in losing segments) |\n| **Rep Performance Variance** | Whether it's a team problem or an individual problem |\n| **Deal Size vs. Win Rate Curve** | The \"sweet spot\" — where does win probability peak? |\n\n### Custom Metrics I'll Introduce (detailed in Part 2)\n1. **Rep-Segment Fit Score (RSFS):** For each deal, how well does the assigned rep match the deal's industry based on historical win patterns? This captures the execution dimension — not what the deal looks like, but whether the right person is working it. If rep-deal fit has degraded, it would explain the win rate decline even with healthy volume.\n\n2. **Segment Momentum Index (SMI):** For each segment (e.g., FinTech, Inbound leads), this captures the *rate of change* of win rate weighted by deal volume. A negative SMI means the segment is both large and deteriorating — the most dangerous combination.\n\n---\n\n## 4. Assumptions\n\n| # | Assumption | Type | Risk if Wrong |\n|---|-----------|------|---------------|\n| 1 | All deals in the dataset have reached a terminal state (Won or Lost). No currently open/active deals. | Data completeness | If there are open deals, survivorship bias could skew win rate calculations for recent periods — recent quarters would appear worse because their open (potentially winnable) deals aren't counted yet. |\n| 2 | The data captures all deals — there are no untracked side-deals or deals in a different CRM. | Data completeness | Missing data would make segment-level analysis unreliable. In practice, CRM data hygiene is a common issue — reps don't always log every deal. |\n| 3 | \"Pipeline looks healthy\" (the CRO's claim) means deal *volume* (count) has been stable or growing. We will verify this with data. | Interpretation of stakeholder input | If the CRO meant something else by \"healthy\" (e.g., stage distribution, deal quality), our validation would be incomplete. |\n| 4 | The 25 sales reps have been relatively stable over the period (no massive hiring wave or attrition). | Business context (not in data) | A large cohort of new reps would naturally depress win rate temporarily — new reps lose more deals as they ramp up. This would be a benign explanation the CRO might not have considered. |\n| 5 | Deal stage at close reflects the furthest stage reached before the deal was won/lost. | Data semantics | If reps update stages inconsistently (skipping stages, or backtracking), any \"where do deals die in the funnel?\" analysis would be misleading. |\n| 6 | External market factors (competitor launches, economic shifts, regulatory changes) are **not captured** in this dataset. The B2B SaaS context is given in the problem statement; what's assumed is that exogenous shocks may exist but are invisible to us. | Analysis boundary | Some portion of the win rate decline may be caused by factors outside the company's control. Our analysis can only diagnose internal/operational factors. |\n| 7 | The CRO is seeking **actionable, near-term insights** (\"what should my team focus on\") rather than a strategic overhaul. This is inferred from their language — \"focus\" implies tactical, current-quarter actions. | Stakeholder intent | If the CRO actually wants a 12-month strategic plan, our analysis (which optimizes for quick wins like lead source audits and rep coaching) would be too narrow. |\n\n---\n\n## Framework: How This Connects to the Remaining Parts\n\n```\nPart 1 (This)          Part 2              Part 3              Part 4              Part 5\n┌────────────┐    ┌────────────┐   ┌────────────┐   ┌────────────┐   ┌────────────┐\n│  Business   │───>│  Validate   │──>│ Operatio-  │──>│ Productize │──>│  Honest    │\n│  Hypotheses │    │  with Data  │   │ nalize     │   │ the System │   │  Assessment│\n└────────────┘    └────────────┘   └────────────┘   └────────────┘   └────────────┘\nWHY is this      WHAT does the   HOW do we        WHAT would       WHERE are we\nhappening?       data confirm?   score & rank?    this look like   least sure?\n                                                  at scale?\n```\n\nEach part builds on the previous one. We don't jump to modeling (Part 3) until we understand the problem (Part 1) and have evidence (Part 2)."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}